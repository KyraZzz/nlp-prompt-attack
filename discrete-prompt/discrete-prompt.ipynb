{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "019dcbd0",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3758580",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73980eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "411c59f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, AdamW, AutoConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0ecc82",
   "metadata": {},
   "source": [
    "# Download datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e3dc71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "165f66cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db40c1668a2c4a5babd0339e6262e3f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.14k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e870b7a0aa9b463fb8fa0c702c2434cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/2.88k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "/jmain02/home/J2AD015/axf03/yxz79-axf03/.conda/envs/nlp-prompt-attack-env/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset multi_nli/default (download: 216.34 MiB, generated: 410.92 MiB, post-processed: Unknown size, total: 627.27 MiB) to /jmain02/home/J2AD015/axf03/yxz79-axf03/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a44734e22b7b49779aaefc27eb4a8208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/227M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/392702 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation_matched split:   0%|          | 0/9815 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation_mismatched split:   0%|          | 0/9832 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset multi_nli downloaded and prepared to /jmain02/home/J2AD015/axf03/yxz79-axf03/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa0a7db78d684a59958b4a8f44b3b315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if mnli_path == None:\n",
    "    mnli = load_dataset(\"multi_nli\")\n",
    "    mnli_path = \"./datasets/mnli\"\n",
    "    mnli.save_to_disk(mnli_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3663e374",
   "metadata": {},
   "outputs": [],
   "source": [
    "qnli_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7915d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration SetFit--qnli-324fd6914ad1beff\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/SetFit--qnli to /jmain02/home/J2AD015/axf03/yxz79-axf03/.cache/huggingface/datasets/SetFit___json/SetFit--qnli-324fd6914ad1beff/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b453872fd2745d4821f6285d988d970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0944bf8c4fbe47d482ece67b5709cea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/31.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "176905a5bd2e481ca790c012f55dffc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.66M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6988818c832346e9b141568c838e9c7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.66M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a8052778be84329aa1e732ee386e885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /jmain02/home/J2AD015/axf03/yxz79-axf03/.cache/huggingface/datasets/SetFit___json/SetFit--qnli-324fd6914ad1beff/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "253925afcca44790b27f05327d321f3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if qnli_path == None:\n",
    "    qnli = load_dataset(\"SetFit/qnli\")\n",
    "    qnli_path = \"./datasets/qnli\"\n",
    "    qnli.save_to_disk(qnli_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe49be5",
   "metadata": {},
   "source": [
    "# Inspect datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b43448",
   "metadata": {},
   "source": [
    "## For qnli dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e00543fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text1', 'text2', 'label', 'idx', 'label_text'],\n",
       "        num_rows: 104743\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text1', 'text2', 'label', 'idx', 'label_text'],\n",
       "        num_rows: 5463\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text1', 'text2', 'label', 'idx', 'label_text'],\n",
       "        num_rows: 5463\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qnli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b79a1f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "qnli_train = qnli['train']\n",
    "qnli_test = qnli['test']\n",
    "qnli_val = qnli['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41480f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text1': ['What individual was responsible for law and maintaining order in the county?',\n",
       "  'How much of the gross domestic product was spent on public health in 2004?'],\n",
       " 'text2': ['He was the top civil and military leader of the commandery and handled defense, lawsuits, seasonal instructions to farmers and recommendations of nominees for office sent annually to the capital in a quota system first established by Emperor Wu.',\n",
       "  'Public expenditure health was at 8.9% of the GDP in 2004, whereas private expenditure was at 1.3%.'],\n",
       " 'label': [1, 0],\n",
       " 'idx': [104741, 104742],\n",
       " 'label_text': ['not entailment', 'entailment']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label 0 - entailment, label 1 - not entailment\n",
    "qnli_train[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5abbe76c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What individual was responsible for law and maintaining order in the county?',\n",
       " 'How much of the gross domestic product was spent on public health in 2004?']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qnli_train['text1'][-2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcea50ed",
   "metadata": {},
   "source": [
    "## For mnli dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72e1d1e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['promptID', 'pairID', 'premise', 'premise_binary_parse', 'premise_parse', 'hypothesis', 'hypothesis_binary_parse', 'hypothesis_parse', 'genre', 'label'],\n",
       "        num_rows: 392702\n",
       "    })\n",
       "    validation_matched: Dataset({\n",
       "        features: ['promptID', 'pairID', 'premise', 'premise_binary_parse', 'premise_parse', 'hypothesis', 'hypothesis_binary_parse', 'hypothesis_parse', 'genre', 'label'],\n",
       "        num_rows: 9815\n",
       "    })\n",
       "    validation_mismatched: Dataset({\n",
       "        features: ['promptID', 'pairID', 'premise', 'premise_binary_parse', 'premise_parse', 'hypothesis', 'hypothesis_binary_parse', 'hypothesis_parse', 'genre', 'label'],\n",
       "        num_rows: 9832\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "260f7ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_train = mnli['train']\n",
    "mnli_val_match = mnli['validation_matched']\n",
    "mnli_val_mismatch = mnli['validation_mismatched']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "341ca12a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['promptID', 'pairID', 'premise', 'premise_binary_parse', 'premise_parse', 'hypothesis', 'hypothesis_binary_parse', 'hypothesis_parse', 'genre', 'label'],\n",
       "    num_rows: 392702\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnli_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b7d8c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'promptID': 31193,\n",
       " 'pairID': '31193n',\n",
       " 'premise': 'Conceptually cream skimming has two basic dimensions - product and geography.',\n",
       " 'premise_binary_parse': '( ( Conceptually ( cream skimming ) ) ( ( has ( ( ( two ( basic dimensions ) ) - ) ( ( product and ) geography ) ) ) . ) )',\n",
       " 'premise_parse': '(ROOT (S (NP (JJ Conceptually) (NN cream) (NN skimming)) (VP (VBZ has) (NP (NP (CD two) (JJ basic) (NNS dimensions)) (: -) (NP (NN product) (CC and) (NN geography)))) (. .)))',\n",
       " 'hypothesis': 'Product and geography are what make cream skimming work. ',\n",
       " 'hypothesis_binary_parse': '( ( ( Product and ) geography ) ( ( are ( what ( make ( cream ( skimming work ) ) ) ) ) . ) )',\n",
       " 'hypothesis_parse': '(ROOT (S (NP (NN Product) (CC and) (NN geography)) (VP (VBP are) (SBAR (WHNP (WP what)) (S (VP (VBP make) (NP (NP (NN cream)) (VP (VBG skimming) (NP (NN work)))))))) (. .)))',\n",
       " 'genre': 'government',\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnli_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97c2a49b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'promptID': 37397,\n",
       " 'pairID': '37397e',\n",
       " 'premise': 'How do you know? All this is their information again.',\n",
       " 'premise_binary_parse': '( ( How ( ( ( do you ) know ) ? ) ) ( ( All this ) ( ( ( is ( their information ) ) again ) . ) ) )',\n",
       " 'premise_parse': '(ROOT (S (SBARQ (WHADVP (WRB How)) (SQ (VBP do) (NP (PRP you)) (VP (VB know))) (. ?)) (NP (PDT All) (DT this)) (VP (VBZ is) (NP (PRP$ their) (NN information)) (ADVP (RB again))) (. .)))',\n",
       " 'hypothesis': 'This information belongs to them.',\n",
       " 'hypothesis_binary_parse': '( ( This information ) ( ( belongs ( to them ) ) . ) )',\n",
       " 'hypothesis_parse': '(ROOT (S (NP (DT This) (NN information)) (VP (VBZ belongs) (PP (TO to) (NP (PRP them)))) (. .)))',\n",
       " 'genre': 'fiction',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnli_train[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6412521",
   "metadata": {},
   "source": [
    "# Preprocess dataset QNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3f03942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# roberta-base or roberta-large\n",
    "PLM = \"roberta-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99464e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "107e7cb31cf94cc5a7e51052b4b982c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fbf4a99209d4204b1957680003aabad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model = AutoModel.from_pretrained(PLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f547cc8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "902a151f1e7e458cbf1c3868d27df60f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44c0e70167244c5d8b6264c440396049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44556f413d784d4897b56f17c0e714af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load tokeniser for the specific model\n",
    "tokeniser = AutoTokenizer.from_pretrained(PLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "847b2116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0, 45093,  2788,     7,  1296,   480, 19233,  6315,   646,  1001,\n",
      "          6747,   102,    12, 11070,   742,     2]])\n"
     ]
    }
   ],
   "source": [
    "# test the tokeniser\n",
    "test_input = \"Simple text to test -- tokenizer [roberta-base]\"\n",
    "tokenised_test_input = tokeniser.encode(test_input, return_tensors=\"pt\")\n",
    "print(tokenised_test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92e7e328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'Simple', 'Ġtext', 'Ġto', 'Ġtest', 'Ġ--', 'Ġtoken', 'izer', 'Ġ[', 'ro', 'bert', 'a', '-', 'base', ']', '</s>']\n"
     ]
    }
   ],
   "source": [
    "test_token_id_to_text = tokeniser.convert_ids_to_tokens(tokeniser.encode(test_input))\n",
    "print(test_token_id_to_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb65310c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model\n",
    "test_output = model(tokenised_test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e6b4cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 16, 768]), torch.Size([1, 768]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "First output:\n",
    "# (1,16,768): batch_size * #tokens * embedding_size_defined_by_model\n",
    "# our input is one sentence with 16 tokens\n",
    "Second output - pooler output, the embedding result of the first token of the sequence <s>\n",
    "\"\"\"\n",
    "test_output[0].size(), test_output[1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "873e3e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_prompt(questions, answers, template):\n",
    "    prompt = questions + template + answers\n",
    "    prompt_token_id = tokeniser(prompt, return_tensors=\"pt\").input_ids\n",
    "    prompt_id_to_text = tokeniser.convert_ids_to_tokens(prompt_token_id[0])\n",
    "    mask_token_pos = prompt_id_to_text.index(tokeniser.mask_token)\n",
    "    return prompt_token_id, mask_token_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cdb15512",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_template = \" <mask>. \"\n",
    "prompt_token_id, mask_token_pos = manual_prompt(qnli_train[0]['text1'], qnli_train[0]['text2'], manual_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1941df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 62, 768])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(prompt_token_id)[0]\n",
    "print(predictions.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a096df1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "values, indices = torch.sort(predictions[0, mask_token_pos], descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dce1cb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = list(zip(tokeniser.convert_ids_to_tokens(indices), values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e2d52a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Ġreal', tensor(10.4360))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e7750573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qnli_train[0]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a8287e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether dataset is balanced\n",
    "def is_balanced(dataset, threshold=0.1):\n",
    "    all_labels = torch.tensor(dataset)\n",
    "    ratio = all_labels.sum() / len(all_labels)\n",
    "    return abs(2 * ratio - 1) < 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e91d7bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(True), tensor(True), tensor(True))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_balanced(qnli_train['label']), is_balanced(qnli_test['label']), is_balanced(qnli_val['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "db4be693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 'not entailment')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qnli_train[0]['label'], qnli_train[0]['label_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5bd88a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(270, 2458)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the max length of question and answer\n",
    "max([len(i) for i in qnli_train['text1']]), max([len(i) for i in qnli_train['text2']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88882c4",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "25366573",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PARAMS = {\n",
    "    \"batch_size\": 24,\n",
    "    \"lr\": 1e-3,\n",
    "    \"max_epochs\": 1,\n",
    "    \"model_name\": \"roberta-base\",\n",
    "    \"max_token_count\": 2048,\n",
    "    \"random_seed\": 42\n",
    "}\n",
    "pl.seed_everything(PARAMS['random_seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2dab00f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class QNLIDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_token_len, template):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.max_token_len = max_token_len\n",
    "        self.template = template\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data.iloc[index]\n",
    "        question = row.text1\n",
    "        answer = row.text2\n",
    "        label = row.label\n",
    "        label_text = row.label_text\n",
    "        \n",
    "        # prompt = <question><SEP><template><answer><SEP> \n",
    "        prompt = question + \" \" + template + \", \" + answer\n",
    "        end_prompt = self.tokenizer.encode_plus(\n",
    "            prompt,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_token_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return dict(\n",
    "            prompt=prompt,\n",
    "            input_ids=end_prompt[\"input_ids\"].flatten(),\n",
    "            attention_mask=end_prompt[\"attention_mask\"].flatten(),\n",
    "            label=torch.FloatTensor(label),\n",
    "            label_text=label_text\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "57755a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "class QNLIDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_data, val_data, test_data, tokenizer, template, batch_size, max_token_len):\n",
    "        super().__init__()\n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.test_data = test_data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_size = batch_size\n",
    "        self.max_token_len = max_token_len\n",
    "        \n",
    "    def setup(self):\n",
    "        self.train_dataset = QNLIDataset(\n",
    "            self.train_data,\n",
    "            self.tokenizer,\n",
    "            self.max_token_len,\n",
    "            self.template\n",
    "        )\n",
    "        \n",
    "        self.val_dataset = QNLIDataset(\n",
    "            self.val_data,\n",
    "            self.tokenizer,\n",
    "            self.max_token_len,\n",
    "            self.template\n",
    "        )\n",
    "        \n",
    "        self.test_dataset = QNLIDataset(\n",
    "            self.test_data,\n",
    "            self.tokenizer,\n",
    "            self.max_token_len,\n",
    "            self.template\n",
    "        )\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return Dataloader(\n",
    "            self.train_dataset,\n",
    "            batch_size = self.batch_size,\n",
    "            shuffle = True,\n",
    "            num_workers = 2\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return Dataloader(\n",
    "            self.val_dataset,\n",
    "            batch_size = self.batch_size,\n",
    "            shuffle = True,\n",
    "            num_workers = 2\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return Dataloader(\n",
    "            self.test_dataset,\n",
    "            batch_size = self.batch_size,\n",
    "            shuffle = True,\n",
    "            num_workers = 2\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e8ead402",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "class QNLIClassifier(pl.LightningModule):\n",
    "    def __init__(self, n_classes: int, n_training_steps=None, n_warmup_steps=None):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(PARAMS['model_name'], return_dict=True)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "        self.n_training_steps = n_training_steps\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.criterion = nn.BCELoss()\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        output = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        output = self.classifier(output.pooler_output)\n",
    "        output = torch.sigmoid(output)\n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(output, labels)\n",
    "        return loss, output\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "        return {\"loss\": loss, \"predictions\": outputs, \"labels\": labels}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        labels = []\n",
    "        predictions = []\n",
    "        for output in outputs:\n",
    "            for out_labels in output[\"labels\"].detach().cpu():\n",
    "                labels.append(out_labels)\n",
    "            for out_predictions in output[\"predictions\"].detach().cpu():\n",
    "                predictions.append(out_predictions)\n",
    "        \n",
    "        labels = torch.stack(labels).int()\n",
    "        predictions = torch.stack(predictions)\n",
    "        \n",
    "        class_roc_auc = auroc(predictions[:, 0], labels[:, 0])\n",
    "        self.logger.experiment.add_scalar(f\"{label}_roc_auc/Train\", class_roc_auc, self.current_epoch)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=PARAMS['lr'])\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.n_warmup_steps,\n",
    "            num_training_steps=self.n_training_steps\n",
    "        ) # learning rate scheduler\n",
    "        \n",
    "        return dict(\n",
    "            optimizer=optimizer,\n",
    "            lr_scheduler=dict(\n",
    "                scheduler=scheduler,\n",
    "                interval='step'\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "89a03199",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import torch.nn as nn\n",
    "def prepare_and_train(train_df, val_df, test_df, template):\n",
    "    \"\"\" Training best practices\n",
    "        - Checkpointing that saves the best model based on validation loss\n",
    "        - Logging the progress in TensorBoard\n",
    "        - Early stopping that terminates the training when the loss has not improved for the last 2 epochs\n",
    "    \"\"\"\n",
    "    logger = TensorBoardLogger('/local/scratch-3/yz709/nlp-prompt-attack/tb_logs', name='discrete-prompt-roberta-base')\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath='checkpoints',\n",
    "        filename='discrete-prompt-roberta-base-{epoch:02d}-{val_loss:.2f}',\n",
    "        verbose=True,\n",
    "        monitor='val_loss',\n",
    "        mode='min'\n",
    "    )\n",
    "    early_stopping_callback = EarlyStopping(monitor='val_loss', patience=2)\n",
    "    \n",
    "    # preprocess data\n",
    "    tokenizer = AutoTokenizer.from_pretrained(PARAMS['model_name'])\n",
    "    train_dataset = QNLIDataset(train_df, tokenizer, PARAMS['max_token_count'], template)\n",
    "    data_module = QNLIDataModule(\n",
    "        train_df,\n",
    "        val_df,\n",
    "        test_df,\n",
    "        tokenizer,\n",
    "        template,\n",
    "        batch_size=PARAMS['batch_size'],\n",
    "        max_token_len=PARAMS['max_token_count']\n",
    "    )\n",
    "\n",
    "    # model\n",
    "    steps_per_epoch = len(train_df) // PARAMS['batch_size']\n",
    "    total_training_steps = steps_per_epoch * PARAMS['max_epochs']\n",
    "    warmup_steps = total_training_steps // 5\n",
    "    model = QNLIClassifier(\n",
    "        n_classes=2,\n",
    "        n_warmup_steps=warmup_steps,\n",
    "        n_training_steps=total_training_steps\n",
    "    )\n",
    "    \n",
    "    # train\n",
    "    trainer = pl.Trainer(\n",
    "        logger = logger,\n",
    "        callbacks=[early_stopping_callback,checkpoint_callback],\n",
    "        max_epochs=PARAMS['max_epochs'],\n",
    "        accelerator=\"gpu\", \n",
    "        devices=[1,2,3],\n",
    "        strategy=\"ddp\",\n",
    "    )\n",
    "    trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f5651f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "MisconfigurationException",
     "evalue": "No supported gpu backend found!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [85]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprepare_and_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqnli_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqnli_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqnli_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m <mask> \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [84]\u001b[0m, in \u001b[0;36mprepare_and_train\u001b[0;34m(train_df, val_df, test_df, template)\u001b[0m\n\u001b[1;32m     37\u001b[0m model \u001b[38;5;241m=\u001b[39m QNLIClassifier(\n\u001b[1;32m     38\u001b[0m     n_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     39\u001b[0m     n_warmup_steps\u001b[38;5;241m=\u001b[39mwarmup_steps,\n\u001b[1;32m     40\u001b[0m     n_training_steps\u001b[38;5;241m=\u001b[39mtotal_training_steps\n\u001b[1;32m     41\u001b[0m )\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcheckpoint_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPARAMS\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_epochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mddp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(model, data_module)\n",
      "File \u001b[0;32m~/.conda/envs/nlp-prompt-attack-env/lib/python3.10/site-packages/pytorch_lightning/utilities/argparse.py:345\u001b[0m, in \u001b[0;36m_defaults_from_env_vars.<locals>.insert_env_defaults\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mlist\u001b[39m(env_variables\u001b[38;5;241m.\u001b[39mitems()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mitems()))\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# all args were already moved to kwargs\u001b[39;00m\n\u001b[0;32m--> 345\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/nlp-prompt-attack-env/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:433\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, logger, enable_checkpointing, callbacks, default_root_dir, gradient_clip_val, gradient_clip_algorithm, num_nodes, num_processes, devices, gpus, auto_select_gpus, tpu_cores, ipus, enable_progress_bar, overfit_batches, track_grad_norm, check_val_every_n_epoch, fast_dev_run, accumulate_grad_batches, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, val_check_interval, log_every_n_steps, accelerator, strategy, sync_batchnorm, precision, enable_model_summary, weights_save_path, num_sanity_val_steps, resume_from_checkpoint, profiler, benchmark, deterministic, reload_dataloaders_every_n_epochs, auto_lr_find, replace_sampler_ddp, detect_anomaly, auto_scale_batch_size, plugins, amp_backend, amp_level, move_metrics_to_cpu, multiple_trainloader_mode)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# init connectors\u001b[39;00m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_connector \u001b[38;5;241m=\u001b[39m DataConnector(\u001b[38;5;28mself\u001b[39m, multiple_trainloader_mode)\n\u001b[0;32m--> 433\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerator_connector \u001b[38;5;241m=\u001b[39m \u001b[43mAcceleratorConnector\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_processes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_processes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtpu_cores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtpu_cores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m    \u001b[49m\u001b[43mipus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mipus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m    \u001b[49m\u001b[43msync_batchnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msync_batchnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbenchmark\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbenchmark\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreplace_sampler_ddp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreplace_sampler_ddp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauto_select_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauto_select_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamp_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamp_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamp_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamp_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplugins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplugins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger_connector \u001b[38;5;241m=\u001b[39m LoggerConnector(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_connector \u001b[38;5;241m=\u001b[39m CallbackConnector(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/nlp-prompt-attack-env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:212\u001b[0m, in \u001b[0;36mAcceleratorConnector.__init__\u001b[0;34m(self, devices, num_nodes, accelerator, strategy, plugins, precision, amp_type, amp_level, sync_batchnorm, benchmark, replace_sampler_ddp, deterministic, auto_select_gpus, num_processes, tpu_cores, ipus, gpus)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerator_flag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_choose_auto_accelerator()\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerator_flag \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerator_flag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_choose_gpu_accelerator_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_parallel_devices_and_init_accelerator()\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m# 3. Instantiate ClusterEnvironment\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/nlp-prompt-attack-env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:518\u001b[0m, in \u001b[0;36mAcceleratorConnector._choose_gpu_accelerator_backend\u001b[0;34m()\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m CUDAAccelerator\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 518\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo supported gpu backend found!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mMisconfigurationException\u001b[0m: No supported gpu backend found!"
     ]
    }
   ],
   "source": [
    "prepare_and_train(qnli_train, qnli_val, qnli_test, template = \" <mask> \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c72723",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dec6bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b97383",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
